{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitdeeplearningcondaeffde72656b44ccc9f803edcd0e403e5",
   "display_name": "Python 3.7.3 64-bit ('deep-learning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "18a2e600c03f756d0e3d157c29057f1563464d8816fdf409e1be001e9561fe0e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4\n"
    }
   ],
   "source": [
    "topics = open('./Data/topics.txt', 'r',encoding='utf-8')\n",
    "\n",
    "dataframes = []\n",
    "for filename in topics:\n",
    "    filename = filename.strip()\n",
    "    data = pd.read_csv('./Data/CSV/Training/'+filename+'.csv')\n",
    "    classification = []\n",
    "    for i in range(data.shape[0]):\n",
    "        classification.append(filename)\n",
    "        text = data['Body'][i]\n",
    "        if text == None or str(text) == 'nan' or len(text) == 0:\n",
    "            continue\n",
    "        #print(\"===Raw Text:===\\n\", text)\n",
    "\n",
    "        #Lowercase the text\n",
    "        text = text.lower()\n",
    "        #print(\"\\n===After Lowercase:===\\n\", text)\n",
    "\n",
    "        #HTML Tag removal\n",
    "        text = BeautifulSoup(text).get_text()\n",
    "        #print(\"\\n===HTML Tag Removed:===\\n\", text)\n",
    "\n",
    "\n",
    "        #Number Removal\n",
    "        \n",
    "        text = re.sub(r'[-+]?\\d+', '', text)\n",
    "        #print(\"\\n===After Removing Numbers:===\\n\", text)\n",
    "\n",
    "        #Remove punctuations\n",
    "        text=text.translate((str.maketrans('','',string.punctuation)))\n",
    "        #print(\"\\n===After Removing Punctuations:===\\n\", text)\n",
    "\n",
    "        # #Tokenize\n",
    "        # text = word_tokenize(text)\n",
    "        # print(\"\\n===After Tokenizing:===\\n\", text)\n",
    "\n",
    "        # #Remove stopwords\n",
    "        # stop_words = set(stopwords.words('english'))\n",
    "        # text = [word for word in text if not word in stop_words]\n",
    "        # print(\"\\n===After Stopword Removal:===\\n\", text)\n",
    "\n",
    "        # #Lemmatize tokens\n",
    "        # lemmatizer=WordNetLemmatizer()\n",
    "        # text = [lemmatizer.lemmatize(word) for word in text]\n",
    "        # print(\"\\n===After Lemmatization:===\\n\", text)\n",
    "\n",
    "        # #Stemming tokens\n",
    "        # stemmer= PorterStemmer()\n",
    "        # text = [stemmer.stem(word) for word in text]\n",
    "        # print(\"\\n===After Stemming:===\\n\", text)\n",
    "        data['Body'][i] = text\n",
    "        \n",
    "    data['Topic']= classification\n",
    "    dataframes.append(data)\n",
    "print(len(dataframes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(36377, 2)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "trainset = pd.concat(dataframes,ignore_index=True)\n",
    "trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.to_csv('./Data/train.csv',index=False)"
   ]
  }
 ]
}